{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fce5b49a",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c455365",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers\n",
    "import pandas as pd\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import ollama\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6a02d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02610209",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv(\"../../script/data_annotation/Attack_scenarios_350_tokens.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907af0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_examples = df[['title', 'rounded_average_int', 'news_cleaned_350']]\n",
    "df_examples = df_examples[df_examples['rounded_average_int'] != 'neither believes nor disbelieves']\n",
    "df_examples['rounded_average_int'] = df_examples['rounded_average_int'].str.replace('completely ', '', regex=False)\n",
    "df_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ccf190",
   "metadata": {},
   "source": [
    "### Naive prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0abe271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def classify_with_ollama(news_article: str) -> str:\n",
    "    \"\"\"\n",
    "    Classify a single news article into 'believes' or 'disbelieves' using Ollama (naive, zero-shot).\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Read the following news article and classify it into exactly one category:\n",
    "\n",
    "- believes\n",
    "- disbelieves\n",
    "\n",
    "Rules:\n",
    "- Answer with ONLY the label, no explanation.\n",
    "\n",
    "News: \"{news_article}\"\n",
    "\n",
    "Label:\n",
    "\"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3.1:8b\",  # change this to any Llama model you want\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    prediction = response.get(\"message\", {}).get(\"content\", \"\").strip().lower()\n",
    "    if prediction not in {\"believes\", \"disbelieves\"}:\n",
    "        prediction = \"disbelieves\"  # fallback default\n",
    "    return prediction\n",
    "\n",
    "\n",
    "# ---- Apply to your DataFrame ----\n",
    "tqdm.pandas()\n",
    "df_examples[\"belief_classification\"] = df_examples[\"news_cleaned_350\"].progress_apply(classify_with_ollama)\n",
    "\n",
    "print(df_examples[[\"news_cleaned_350\", \"belief_classification\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db247eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Sequence, Tuple, Dict, Any\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "def compute_label_metrics(\n",
    "    df: pd.DataFrame,\n",
    "    true_col: str,\n",
    "    pred_col: str,\n",
    "    classes: Sequence[str] = (\"disbelieves\", \"believes\"),\n",
    "    nan_placeholder: str = \"__nan__\",       # what we map NaN/empty preds to\n",
    "    normalize: bool = True                  # lowercase/strip both columns\n",
    ") -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Compute classification metrics while KEEPING all rows.\n",
    "    Any NaN/empty prediction is treated as a mismatch (mapped to `nan_placeholder`).\n",
    "\n",
    "    Returns:\n",
    "      results_df: 1-row DataFrame with:\n",
    "        support_disbelieves, support_believes,\n",
    "        mis_disbelieves, mis_believes,\n",
    "        correct_disbelieves, correct_believes,\n",
    "        accuracy, f1_disbelieves, f1_believes, f1_macro, f1_weighted, total_samples\n",
    "      details: dict containing confusion matrix, per-class report, counts, etc.\n",
    "    \"\"\"\n",
    "    # Copy to avoid mutating caller data\n",
    "    d = df[[true_col, pred_col]].copy()\n",
    "\n",
    "    # Normalize text (safe if already clean)\n",
    "    if normalize:\n",
    "        d[true_col] = d[true_col].astype(str).str.strip().str.lower()\n",
    "        # Note: \"nan\" as a string may appear after astype(str); convert to np.nan before filling\n",
    "        pred_norm = d[pred_col].astype(str).str.strip().str.lower()\n",
    "        pred_norm = pred_norm.replace({\"\": nan_placeholder, \"nan\": nan_placeholder})\n",
    "        d[pred_col] = pred_norm\n",
    "    else:\n",
    "        # Only map NaN in preds to placeholder\n",
    "        d[pred_col] = d[pred_col].fillna(nan_placeholder)\n",
    "\n",
    "    # Ensure actual NaNs (if any remained) in preds become placeholder\n",
    "    d[pred_col] = d[pred_col].fillna(nan_placeholder)\n",
    "\n",
    "    y_true = d[true_col].values\n",
    "    y_pred = d[pred_col].values\n",
    "\n",
    "    # --- Metrics ---\n",
    "    # Accuracy over ALL rows; placeholder ensures NaN preds are mismatches\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Per-class report (only over REAL classes, exclude placeholder from averaging)\n",
    "    report = classification_report(\n",
    "        y_true, y_pred, labels=list(classes), output_dict=True, zero_division=0\n",
    "    )\n",
    "\n",
    "    # Per-class F1\n",
    "    f1_per_class = {c: report[c][\"f1-score\"] for c in classes}\n",
    "\n",
    "    # Macro / weighted across REAL classes\n",
    "    f1_macro = f1_score(y_true, y_pred, labels=list(classes), average=\"macro\", zero_division=0)\n",
    "    f1_weighted = f1_score(y_true, y_pred, labels=list(classes), average=\"weighted\", zero_division=0)\n",
    "\n",
    "    # Confusion matrix ONLY for REAL classes (rows=true, cols=pred)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(classes))\n",
    "\n",
    "    # Supports from y_true (per real class)\n",
    "    supports = pd.Series(y_true).value_counts().reindex(list(classes), fill_value=0)\n",
    "\n",
    "    # Correct / misclass counts\n",
    "    correct = {\n",
    "        classes[0]: int(cm[0, 0]),\n",
    "        classes[1]: int(cm[1, 1]),\n",
    "    }\n",
    "    mis = {\n",
    "        classes[0]: int(supports[classes[0]] - correct[classes[0]]),\n",
    "        classes[1]: int(supports[classes[1]] - correct[classes[1]]),\n",
    "    }\n",
    "\n",
    "    results = {\n",
    "        f\"support_{classes[0]}\": int(supports[classes[0]]),\n",
    "        f\"support_{classes[1]}\": int(supports[classes[1]]),\n",
    "        f\"mis_{classes[0]}\":     mis[classes[0]],\n",
    "        f\"mis_{classes[1]}\":     mis[classes[1]],\n",
    "        f\"correct_{classes[0]}\": correct[classes[0]],\n",
    "        f\"correct_{classes[1]}\": correct[classes[1]],\n",
    "        \"accuracy\":              accuracy,\n",
    "        f\"f1_{classes[0]}\":      f1_per_class[classes[0]],\n",
    "        f\"f1_{classes[1]}\":      f1_per_class[classes[1]],\n",
    "        \"f1_macro\":              f1_macro,\n",
    "        \"f1_weighted\":           f1_weighted,\n",
    "        \"total_samples\":         len(d),\n",
    "    }\n",
    "\n",
    "    details = {\n",
    "        \"confusion_matrix_labels\": list(classes),\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"classification_report\": report,\n",
    "        \"supports\": supports.to_dict(),\n",
    "        \"correct\": correct,\n",
    "        \"mis\": mis,\n",
    "        \"num_rows_with_nan_pred\": int((df[pred_col].isna()).sum()),\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame([results]), details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a068d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df, details = compute_label_metrics(\n",
    "    df_examples,\n",
    "    true_col=\"rounded_average_int\",\n",
    "    pred_col=\"belief_classification\",\n",
    "    classes=(\"disbelieves\", \"believes\")  # change if your labels differ\n",
    ")\n",
    "\n",
    "print(results_df)          # one-row table of your requested metrics\n",
    "print(details[\"confusion_matrix_labels\"])\n",
    "print(details[\"confusion_matrix\"])       # rows=true, cols=pred in the order above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80697f0",
   "metadata": {},
   "source": [
    "### COLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c682fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def linguist_analysis_with_ollama(news_text):\n",
    "    prompt = \"\"\"\n",
    "You are a linguist trained to analyze written language.\n",
    "\n",
    "Accurately and concisely explain the linguistic elements in the following sentence, including:\n",
    "- grammatical structure,\n",
    "- tense and inflection,\n",
    "- figurative or virtual speech,\n",
    "- rhetorical devices,\n",
    "- lexical choices,\n",
    "and how these elements affect meaning.\n",
    "\n",
    "Only provide linguistic analysis. Do not summarize the content or give irrelevant commentary.\n",
    "\n",
    "Text: \"{}\"\n",
    "\"\"\".format(news_text)\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3.1:8b\",  # Adjust the model tag if needed, like \"llama3:70b\"\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.get(\"message\", {}).get(\"content\", \"No response received\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b2c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable progress bar support for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Track runtime\n",
    "start_time = time.time()\n",
    "\n",
    "# Apply the function with progress bar\n",
    "df_examples['linguistic_analysis'] = df_examples['news_cleaned'].progress_apply(linguist_analysis_with_ollama)\n",
    "\n",
    "# Print total time taken\n",
    "end_time = time.time()\n",
    "print(f\"Total time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b95f4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expert_analysis_with_ollama(news_text):\n",
    "    instruction = (\n",
    "        \"You are a political scientist trained to analyze political content in text.\\n\\n\"\n",
    "        \"Accurately and concisely explain the key elements in the following quote, such as:\\n\"\n",
    "        \"- characters\\n\"\n",
    "        \"- events\\n\"\n",
    "        \"- political parties\\n\"\n",
    "        \"- organizations\\n\"\n",
    "        \"- religious or ideological references\\n\\n\"\n",
    "        \"Also explain their relevance or relationship to politics (if any). Do nothing else.\"\n",
    "    )\n",
    "\n",
    "    prompt = f\"{instruction}\\n\\nText: \\\"{news_text}\\\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3:8b\",  # or \"llama3:70b\" if you're using the larger version\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.get(\"message\", {}).get(\"content\", \"No response received\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f9d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_examples['expert_analysis'] = df_examples['news_cleaned'].progress_apply(expert_analysis_with_ollama)\n",
    "\n",
    "print(f\"Total time taken: {time.time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed631fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_analysis_with_ollama(text):\n",
    "    # Ensure that the text is a valid string and clean if needed\n",
    "    if not isinstance(text, str):  # Check if text is not a string\n",
    "        text = str(text)  # Convert it to string if necessary\n",
    "    text = text.strip()  # Remove leading/trailing whitespaces\n",
    "\n",
    "    # If the text is empty after cleaning, return a default response\n",
    "    if not text:\n",
    "        return \"No valid content provided\"\n",
    "\n",
    "    instruction = (\n",
    "        \"You are a heavy social media user trained to analyze social content.\\n\\n\"\n",
    "        \"Analyze the following sentence, focusing on:\\n\"\n",
    "        \"- the content\\n\"\n",
    "        \"- hashtags\\n\"\n",
    "        \"- Internet slang and colloquialisms\\n\"\n",
    "        \"- emotional tone\\n\"\n",
    "        \"- implied meaning\\n\\n\"\n",
    "        \"Do nothing else.\"\n",
    "    )\n",
    "\n",
    "    # Construct the full prompt\n",
    "    prompt = f\"{instruction}\\n\\nText: \\\"{text}\\\"\"\n",
    "\n",
    "    # Call the Ollama API\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3:8b\",  # Ensure that you're using the correct model name\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    # Return the response content or a fallback message\n",
    "    return response.get(\"message\", {}).get(\"content\", \"No response received\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e50c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_examples['user_analysis'] = df_examples['news_cleaned'].progress_apply(user_analysis_with_ollama)\n",
    "\n",
    "print(f\"Total time taken: {time.time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc27b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def get_completion(prompt, model=\"llama3:8b\", temperature=0):\n",
    "    \"\"\"\n",
    "    Use Ollama to generate a completion based on a given prompt.\n",
    "    \"\"\"\n",
    "    response = ollama.chat(model=model, messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ])\n",
    "    return response['message']['content'].strip()\n",
    "\n",
    "def believability_analysis(news, ling_response, expert_response, user_response, target): \n",
    "    \"\"\"\n",
    "    Evaluate whether a news item is believable or not, based on different sources of analysis.\n",
    "    `target` should be either 'believes' or 'disbelieves'.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"'''{news}'''\\n\"\n",
    "        f\"<<<{ling_response}>>>\\n\"\n",
    "        f\"[[[{expert_response}]]]\\n\"\n",
    "        f\"---{user_response}---\\n\\n\"\n",
    "        f\"Based on the content provided:\\n\"\n",
    "        f\"- ''' ''' contains the news statement.\\n\"\n",
    "        f\"- <<< >>> contains linguistic cues (e.g., hedging, assertiveness).\\n\"\n",
    "        f\"- [[[ ]]] is the expert analysis (factual correctness, consistency, credibility).\\n\"\n",
    "        f\"- --- --- reflects the perception of a heavy social media user.\\n\\n\"\n",
    "        f\"Do you think the news statement is more aligned with the belief that it is **{target}**?\\n\"\n",
    "        f\"Identify the top three pieces of evidence from the above sources that justify your conclusion, \"\n",
    "        f\"and explain why they support or undermine the believability of the news.\"\n",
    "    )\n",
    "    return get_completion(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf7fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_examples['believes_response'] = df_examples.progress_apply(\n",
    "    lambda row: believability_analysis(\n",
    "        row['news_cleaned'],\n",
    "        row['linguistic_analysis'],\n",
    "        row['expert_analysis'],\n",
    "        row['user_analysis'],\n",
    "        target=\"believes\"\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d018ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_examples['disbelieves_response'] = df_examples.progress_apply(\n",
    "    lambda row: believability_analysis(\n",
    "        row['news_cleaned'],\n",
    "        row['linguistic_analysis'],\n",
    "        row['expert_analysis'],\n",
    "        row['user_analysis'],\n",
    "        target=\"disbelieves\"\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e283c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_judgement(news, believes_response, disbelieves_response):\n",
    "    prompt = (\n",
    "        f\"Determine whether the following news statement is believable or not believable.\\n\\n\"\n",
    "        f\"News: '''{news}'''\\n\\n\"\n",
    "        f\"Judge this in relation to the following arguments:\\n\"\n",
    "        f\"Arguments that the statement is believable: {believes_response}\\n\"\n",
    "        f\"Arguments that the statement is not believable: {disbelieves_response}\\n\\n\"\n",
    "        f\"Choose from:\\n\"\n",
    "        f\"A: Not Believable\\nB: Believable\\n\\n\"\n",
    "        f\"Constraint: Answer with only the option above that is most accurate and nothing else.\"\n",
    "    )\n",
    "\n",
    "    response = ollama.chat(model='llama3:8b', messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ])\n",
    "\n",
    "    return response['message']['content'].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b6407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply final_judgement to decide the believability label\n",
    "df_examples['final_response'] = df_examples.progress_apply(\n",
    "    lambda row: final_judgement(\n",
    "        row['news_cleaned'],\n",
    "        row['believes_response'],\n",
    "        row['disbelieves_response']\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b27b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract only 'A' or 'B' from the noisy LLM response in 'final_response'\n",
    "df_examples['final_response_ab'] = df_examples['final_response'].str.extract(r'\\b([AB])\\b', flags=re.IGNORECASE)\n",
    "\n",
    "# Convert to uppercase for consistency\n",
    "df_examples['final_response_ab'] = df_examples['final_response_ab'].str.upper()\n",
    "\n",
    "# Drop rows where extraction failed (i.e., no A or B found)\n",
    "df_examples = df_examples.dropna(subset=['final_response_ab']).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff681352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Step 1: Convert final model outputs (e.g., A/B) to labels\n",
    "def map_judgement_to_label(x):\n",
    "    if isinstance(x, str):\n",
    "        x = x.strip().upper()\n",
    "        if x == 'A':\n",
    "            return 'disbelieves'\n",
    "        elif x == 'B':\n",
    "            return 'believes'\n",
    "    return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a435cc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Apply the label mapping\n",
    "# df_examples['final_label'] = df_examples['final_response'].apply(map_judgement_to_label)\n",
    "df_examples['final_label'] = df_examples['final_response_ab'].apply(map_judgement_to_label)\n",
    "\n",
    "\n",
    "# Step 2: Remove 'unknown' if any\n",
    "df_filtered = df_examples[df_examples['final_label'] != 'unknown']\n",
    "\n",
    "# Step 3: Evaluate\n",
    "y_true = df_filtered['rounded_average_int']  # your actual column\n",
    "y_pred = df_filtered['final_label']\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['believes', 'disbelieves']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74427db",
   "metadata": {},
   "source": [
    "### In context learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba74f0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate prompt for the news article\n",
    "def generate_prompt(df, row_index, limit):\n",
    "    df_filtered = df.drop(row_index)\n",
    "\n",
    "    def get_first_300_tokens(text, limit):\n",
    "        return text[:limit]    ##### Change the token to 250/550 based on the selected model\n",
    "    \n",
    "    examples = {}\n",
    "\n",
    "    # Sample 10 examples for each label, padding with repeats if not enough\n",
    "    for label in df_filtered['rounded_average_int'].unique():\n",
    "        label_examples = df_filtered[df_filtered['rounded_average_int'] == label]['news_cleaned'].tolist()\n",
    "        \n",
    "        if len(label_examples) >= 10:\n",
    "            # Take 10 random unique samples\n",
    "            sampled_examples = random.sample(label_examples, 10)\n",
    "        else:\n",
    "            # Take all available, then duplicate randomly to reach 10\n",
    "            sampled_examples = label_examples.copy()\n",
    "            needed = 10 - len(sampled_examples)\n",
    "            sampled_examples += random.choices(label_examples, k=needed)\n",
    "\n",
    "        examples[label] = sampled_examples\n",
    "\n",
    "    # Flatten examples into a list\n",
    "    example_list = []\n",
    "    for label in examples:\n",
    "        example_list.extend(examples[label])\n",
    "\n",
    "    # Build example string\n",
    "    example_str = \"\"\n",
    "    for example in example_list:\n",
    "        news_tokens = get_first_300_tokens(example, limit)\n",
    "        label = df_filtered[df_filtered['news_cleaned'] == example]['rounded_average_int'].iloc[0]\n",
    "        example_str += f'News: \"{news_tokens}\"\\nLabel: {label}\\n\\n'\n",
    "    \n",
    "    return example_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c6fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Ollama to classify a new example (using the formatted prompt)\n",
    "def classify_with_ollama(news_article, example_str):\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert trained to classify news articles into 2 categories based on believability: \n",
    "    This news indicates either believing or not into the below defined categories. \n",
    "    For each news, label it as one of the following categories: 'disbelieves', \n",
    "    'believes'. I have provided a few examples below to guide you. \n",
    "    Only respond with the category, no explanation needed\n",
    "\n",
    "    {example_str}\n",
    "\n",
    "    Now classify the following news article:\n",
    "    News: \"{news_article}\"\n",
    "    Label:\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.chat(model=\"llama3.1:8b\", messages=[{\"role\": \"user\", \"content\": prompt}]) ### change model here\n",
    "    \n",
    "    prediction = response.get(\"message\", {}).get(\"content\", \"No response received\").strip()\n",
    "\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7955fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "token_limit = 350 # changed to 225/550/700 depending on the model token limit\n",
    "\n",
    "for index, row in tqdm(df_examples.iterrows(), total=len(df_examples)):\n",
    "    df_cur = row.to_frame().T\n",
    "    gen_string =  generate_prompt(df_examples, index, token_limit)\n",
    "    pre_label = classify_with_ollama(' '.join(df_cur['news_cleaned'].values.astype(str))[:300], gen_string) ### change token depending on how many tokens to select\n",
    "    df_cur['predicted_label'] = pre_label\n",
    "    res.append(df_cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aaea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_preds = pd.concat(res, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Choose the single predicted column to evaluate\n",
    "pred_col = 'predicted_label'  # <-- change this if needed\n",
    "clean_col = f'final_response_cleaned_{pred_col}'\n",
    "\n",
    "ordered_labels = ['disbelieves', 'believes']  # consistent ordering\n",
    "\n",
    "# 1) Clean y_true (string labels)\n",
    "y_true_str = (\n",
    "    df_all_preds['rounded_average_int']\n",
    "    .astype(str).str.lower()\n",
    "    .str.extract(r'\\b(disbelieves|believes)\\b', flags=re.IGNORECASE, expand=False)\n",
    ")\n",
    "\n",
    "metrics = []\n",
    "\n",
    "# 2) Clean the predicted column and evaluate\n",
    "# regex clean predictions to canonical strings\n",
    "df_all_preds[clean_col] = (\n",
    "    df_all_preds[pred_col]\n",
    "    .astype(str).str.lower()\n",
    "    .str.extract(r'\\b(disbelieves|believes)\\b', flags=re.IGNORECASE, expand=False)\n",
    ")\n",
    "\n",
    "# evaluate only where both y_true and y_pred are present\n",
    "mask = y_true_str.notna() & df_all_preds[clean_col].notna()\n",
    "y_t = y_true_str[mask]\n",
    "y_p = df_all_preds[clean_col][mask]\n",
    "n_scored = int(mask.sum())\n",
    "\n",
    "# per-column detailed report\n",
    "print(f\"\\n=== Classification Report for {pred_col} (n={n_scored} rows) ===\")\n",
    "print(classification_report(\n",
    "    y_t, y_p,\n",
    "    labels=ordered_labels,\n",
    "    target_names=ordered_labels,\n",
    "    digits=4,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "if n_scored > 0:\n",
    "    # Confusion matrix (rows=true, cols=pred) in ordered_labels order\n",
    "    cm = confusion_matrix(y_t, y_p, labels=ordered_labels)\n",
    "    # Supports (true counts)\n",
    "    support_dis = int(cm[0, :].sum())\n",
    "    support_bel = int(cm[1, :].sum())\n",
    "    # Misclassifications per true class\n",
    "    mis_dis = int(cm[0, 1])  # true disbelieves predicted as believes\n",
    "    mis_bel = int(cm[1, 0])  # true believes predicted as disbelieves\n",
    "    # Correct counts (diagonal)\n",
    "    corr_dis = int(cm[0, 0])\n",
    "    corr_bel = int(cm[1, 1])\n",
    "\n",
    "    acc      = accuracy_score(y_t, y_p)\n",
    "    f1_dis   = f1_score(y_t, y_p, pos_label='disbelieves', zero_division=0)\n",
    "    f1_bel   = f1_score(y_t, y_p, pos_label='believes',    zero_division=0)\n",
    "    f1_macro = f1_score(y_t, y_p, average='macro',    zero_division=0)\n",
    "    f1_wt    = f1_score(y_t, y_p, average='weighted', zero_division=0)\n",
    "else:\n",
    "    support_dis = support_bel = mis_dis = mis_bel = corr_dis = corr_bel = np.nan\n",
    "    acc = f1_dis = f1_bel = f1_macro = f1_wt = np.nan\n",
    "\n",
    "metrics.append({\n",
    "    'variant': pred_col,\n",
    "    'n_scored': n_scored,\n",
    "    'support_disbelieves': support_dis,\n",
    "    'support_believes': support_bel,\n",
    "    'mis_disbelieves': mis_dis,\n",
    "    'mis_believes': mis_bel,\n",
    "    'correct_disbelieves': corr_dis,\n",
    "    'correct_believes': corr_bel,\n",
    "    'accuracy': acc,\n",
    "    'f1_disbelieves': f1_dis,\n",
    "    'f1_believes': f1_bel,\n",
    "    'f1_macro': f1_macro,\n",
    "    'f1_weighted': f1_wt\n",
    "})\n",
    "\n",
    "# 3) Summary table\n",
    "metrics_df = pd.DataFrame(metrics).sort_values(['f1_macro', 'accuracy'], ascending=False)\n",
    "print(\"\\n=== Summary (sorted by macro-F1, then accuracy) ===\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Optionally save\n",
    "# metrics_df.to_csv(\"metrics_summary_with_misclass_for_news_cleaned.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newllama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
